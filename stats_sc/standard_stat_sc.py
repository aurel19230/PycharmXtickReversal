import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from statsmodels.stats.power import TTestIndPower
from sklearn.feature_selection import f_classif  # Test F (ANOVA)


from definition import *
def plot_distributions_short_long_grid(df, features, class_col='class'):
    """
    Cr√©e une unique figure avec jusqu'√† 24 features.
    - 4 features par ligne => 4 * 2 = 8 colonnes
    - nrows = nombre de lignes n√©cessaire pour afficher toutes les features
    - Pour chaque feature, on a 2 subplots contigus :
         - (Short)  => en bleu/orange (class=0 / class=1)
         - (Long)   => en bleu/orange (class=0 / class=1)
    """

    # 1) Limite √† 24 features (optionnel si d√©j√† fait plus haut)
    max_features = 24
    features = features[:max_features]
    n_features = len(features)

    # 2) On veut 4 features par ligne, chaque feature occupe 2 colonnes
    #    => ncols = 8
    #    => nrows = ceil(n_features / 4)
    ncols = 8
    nrows = int(np.ceil(n_features / 4))

    # 3) Pr√©parer la figure
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols,
                             figsize=(8 * 2, 4 * nrows),  # 8 colonnes * ~2 en largeur, 4 en hauteur par ligne
                             squeeze=False)

    # 4) Boucle sur chaque feature
    for i, feature in enumerate(features):
        # -- Calcul de la ligne et des 2 colonnes (short, long) --
        row = i // 4
        col_short = (i % 4) * 2
        col_long = col_short + 1

        # Subplots correspondants
        ax_short = axes[row, col_short]
        ax_long  = axes[row, col_long]

        # Filtrage pour short / long
        df_short = df[df['pos_type'] == 'short']
        df_long  = df[df['pos_type'] == 'long']

        # === Distribution SHORT ===
        sns.histplot(
            df_short[df_short[class_col] == 0][feature],
            color='blue',
            kde=True,
            label='Class 0',
            ax=ax_short
        )
        sns.histplot(
            df_short[df_short[class_col] == 1][feature],
            color='orange',
            kde=True,
            label='Class 1',
            ax=ax_short
        )
        ax_short.set_title(f"{feature} - SHORT")
        ax_short.set_xlabel('')
        ax_short.set_ylabel('Fr√©quence')
        ax_short.legend()

        # === Distribution LONG ===
        sns.histplot(
            df_long[df_long[class_col] == 0][feature],
            color='blue',
            kde=True,
            label='Class 0',
            ax=ax_long
        )
        sns.histplot(
            df_long[df_long[class_col] == 1][feature],
            color='orange',
            kde=True,
            label='Class 1',
            ax=ax_long
        )
        ax_long.set_title(f"{feature} - LONG")
        ax_long.set_xlabel('')
        ax_long.set_ylabel('Fr√©quence')
        ax_long.legend()

    # 5) Masquer les sous-graphiques inutilis√©s si < 24 features
    used_subplots = n_features * 2  # chaque feature utilise 2 subplots
    total_subplots = nrows * ncols
    for j in range(used_subplots, total_subplots):
        row_empty = j // ncols
        col_empty = j % ncols
        axes[row_empty, col_empty].axis('off')

    plt.tight_layout()
    plt.show()

# ======================
# 4. D√©finition des fonctions de tra√ßage
# ======================
def plot_boxplots(df, features, category_col='trade_category', nrows=3, ncols=4):
    """
    Trace des sns.boxplot pour une liste de 'features' avec un ordre personnalis√©.

    :param df: DataFrame contenant les donn√©es filtr√©es.
    :param features: Liste de features (colonnes) √† tracer.
    :param category_col: Nom de la colonne cat√©gorielle pour l'axe X (ex: 'trade_category').
    :param nrows: Nombre de lignes de subplots.
    :param ncols: Nombre de colonnes de subplots.
    """
    # D√©finir l'ordre des cat√©gories
    custom_order = ["Trades r√©ussis long", "Trades √©chou√©s long", "Trades r√©ussis short", "Trades √©chou√©s short"]

    # Pr√©paration de la figure
    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 3), squeeze=False)

    # Boucle sur chaque feature
    for idx, feature in enumerate(features):
        row = idx // ncols
        col = idx % ncols
        ax = axes[row, col]

        sns.boxplot(
            data=df,
            x=category_col,
            y=feature,
            hue=category_col,
            order=custom_order,  # Appliquer l'ordre personnalis√©
            ax=ax,
            showmeans=True,
            palette="Set2",
            dodge=False
        )

        # Supprime la l√©gende si elle existe
        if ax.legend_ is not None:
            ax.legend_.remove()

        # Personnalisation
        ax.set_title(feature)
        ax.set_xlabel('')
        ax.set_ylabel('Valeur')
        ax.tick_params(axis='x', rotation=30)  # Inclinaison des labels en X

    # Masquer les axes vides si le nombre de features est inf√©rieur √† nrows*ncols
    total_plots = len(features)
    for idx_empty in range(total_plots, nrows * ncols):
        row = idx_empty // ncols
        col = idx_empty % ncols
        axes[row, col].axis('off')

    plt.tight_layout()
    plt.show()



# Fonction pour appliquer les conditions de filtrage
def apply_feature_conditions(df, features_conditions):
    mask = np.ones(len(df), dtype=bool)  # Initialisation du masque global √† True

    for feature, conditions in features_conditions.items():
        # Extraire le nom de base de la feature (pour g√©rer les _range1, _range2, etc.)
        base_feature = feature.split('_range')[0]

        # Filtrer les conditions actives
        active_conditions = [cond for cond in conditions if cond.get('active', False)]
        if not active_conditions:
            continue  # Aucune condition active pour cette feature

        feature_mask = np.zeros(len(df), dtype=bool)  # Initialisation √† False

        for condition in active_conditions:
            if condition['type'] == 'greater_than_or_equal':
                feature_mask |= df[base_feature].fillna(-np.inf) >= condition['threshold']
            elif condition['type'] == 'less_than_or_equal':
                feature_mask |= df[base_feature].fillna(np.inf) <= condition['threshold']
            elif condition['type'] == 'between':
                feature_mask |= df[base_feature].fillna(np.nan).between(
                    condition['min'], condition['max'], inclusive='both'
                )

        mask &= feature_mask  # Intersection avec le masque global

    return df[mask]


def find_consecutive_trades(df, trade_category):
    """
    Trouve les s√©quences cons√©cutives pour une cat√©gorie sp√©cifique de trades,
    en tenant compte des interruptions par d'autres types de trades.
    """
    filtered_df = df.copy()  # On garde tous les trades pour voir les interruptions

    if filtered_df.empty:
        return 0, None, None

    # Conversion des dates
    if not pd.api.types.is_datetime64_any_dtype(filtered_df['date']):
        filtered_df['date'] = pd.to_datetime(filtered_df['date'])

    # Trier par date
    filtered_df = filtered_df.sort_values('date')

    max_sequence = 0
    current_sequence = 0
    max_start_date = None
    max_end_date = None
    current_start_date = None

    import numpy as np  # Assurez-vous que cette ligne existe au d√©but du fichier

    # Trier par date
    filtered_df = filtered_df.sort_values('date')

    max_sequence = 0
    current_sequence = 0
    max_start_date = None
    max_end_date = None
    current_start_date = None

    import numpy as np

    # Convertir la colonne date en tableau numpy d'une mani√®re diff√©rente
    # Cette approche √©vite compl√®tement l'utilisation de to_pydatetime()
    dates = filtered_df['date'].to_numpy()

    # La ligne pour les cat√©gories reste la m√™me
    categories = filtered_df['trade_category'].values

    for i, (date, category) in enumerate(zip(dates, categories)):
        if category == trade_category:
            if current_sequence == 0:
                current_sequence = 1
                current_start_date = date
            else:
                # V√©rifier si le trade pr√©c√©dent √©tait de la m√™me cat√©gorie
                if categories[i - 1] == trade_category:
                    current_sequence += 1
                else:
                    # R√©initialiser si le trade pr√©c√©dent √©tait diff√©rent
                    current_sequence = 1
                    current_start_date = date
        else:
            # Si on trouve la plus longue s√©quence jusqu'ici
            if current_sequence > max_sequence:
                max_sequence = current_sequence
                max_start_date = current_start_date
                max_end_date = dates[i - 1]
            current_sequence = 0

    # V√©rifier la derni√®re s√©quence
    if current_sequence > max_sequence:
        max_sequence = current_sequence
        max_start_date = current_start_date
        max_end_date = dates[-1]

    return max_sequence, max_start_date, max_end_date
def calculate_performance_metrics(df):
    """
    Calcule les m√©triques compl√®tes de performance de trading.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame contenant les donn√©es de trading avec les colonnes:
        - 'trade_pnl' : Profit/Perte de chaque trade
        - 'pos_type' : Type de position ('long' ou 'short')
        - index : dates des trades

    Returns:
    --------
    dict
        Dictionnaire structur√© contenant toutes les m√©triques de performance
    """
    # 1. M√©triques de base sur l'ensemble des trades
    total_trades = len(df)
    winning_trades = df[df['trade_pnl'] > 0]
    losing_trades = df[df['trade_pnl'] < 0]

    num_winning_trades = len(winning_trades)
    num_losing_trades = len(losing_trades)

    # 2. S√©paration et analyse par direction
    # Trades longs
    long_trades = df[df['pos_type'] == 'long']
    winning_trades_long = winning_trades[winning_trades['pos_type'] == 'long']
    losing_trades_long = losing_trades[losing_trades['pos_type'] == 'long']

    num_winning_trades_long = len(winning_trades_long)
    num_losing_trades_long = len(losing_trades_long)
    total_trades_long = num_winning_trades_long + num_losing_trades_long

    # Trades shorts
    short_trades = df[df['pos_type'] == 'short']
    winning_trades_short = winning_trades[winning_trades['pos_type'] == 'short']
    losing_trades_short = losing_trades[losing_trades['pos_type'] == 'short']

    num_winning_trades_short = len(winning_trades_short)
    num_losing_trades_short = len(losing_trades_short)
    total_trades_short = num_winning_trades_short + num_losing_trades_short

    # 3. Calcul des profits et pertes
    gross_profit = winning_trades['trade_pnl'].sum() if not winning_trades.empty else 0
    gross_loss = losing_trades['trade_pnl'].sum() if not losing_trades.empty else 0
    net_pnl = gross_profit + gross_loss

    # Profit factor avec gestion de division par z√©ro
    profit_factor = gross_profit / abs(gross_loss) if abs(gross_loss) > 0 else np.inf if gross_profit > 0 else 0

    # 4. Calcul des win rates
    win_rate = (num_winning_trades / total_trades * 100) if total_trades > 0 else 0
    win_rate_long = (num_winning_trades_long / total_trades_long * 100) if total_trades_long > 0 else 0
    win_rate_short = (num_winning_trades_short / total_trades_short * 100) if total_trades_short > 0 else 0

    # 5. Calcul des moyennes de PnL
    expected_pnl_net = df['trade_pnl'].mean() if not df.empty else 0
    expected_pnl_long = long_trades['trade_pnl'].mean() if not long_trades.empty else 0
    expected_pnl_short = short_trades['trade_pnl'].mean() if not short_trades.empty else 0

    # Moyennes des profits et pertes
    avg_profit_per_win = winning_trades['trade_pnl'].mean() if not winning_trades.empty else 0
    avg_loss_per_loss = losing_trades['trade_pnl'].mean() if not losing_trades.empty else 0

    # 6. Analyse des s√©quences cons√©cutives
    # Analyse des s√©quences cons√©cutives pour les longs
    long_win_seq, long_win_start, long_win_end = find_consecutive_trades(df, "Trades r√©ussis long")
    long_lose_seq, long_lose_start, long_lose_end = find_consecutive_trades(df, "Trades √©chou√©s long")

    # Analyse des s√©quences cons√©cutives pour les shorts
    short_win_seq, short_win_start, short_win_end = find_consecutive_trades(df, "Trades r√©ussis short")
    short_lose_seq, short_lose_start, short_lose_end = find_consecutive_trades(df, "Trades √©chou√©s short")

    # 7. Identification des meilleurs et pires trades
    def get_extreme_trade(trades_df, extreme_type='max'):
        if trades_df.empty:
            return {'PnL': 0, 'Date': None}
        if extreme_type == 'max':
            idx = trades_df['trade_pnl'].idxmax()
            pnl = trades_df['trade_pnl'].max()
        else:
            idx = trades_df['trade_pnl'].idxmin()
            pnl = trades_df['trade_pnl'].min()
        return {'PnL': pnl, 'Date': idx}

    # 8. Construction du dictionnaire de r√©sultats
    return {
        "Total Trades": total_trades,
        "Trades R√©ussis": num_winning_trades,
        "Trades √âchou√©s": num_losing_trades,

        "Trades Longs": {
            "Total": total_trades_long,
            "R√©ussis": num_winning_trades_long,
            "√âchou√©s": num_losing_trades_long,
            "Win Rate": win_rate_long,
            "PnL Moyen": expected_pnl_long,
            "Meilleur Trade": get_extreme_trade(long_trades, 'max'),
            "Pire Trade": get_extreme_trade(long_trades, 'min'),
            "S√©quences Cons√©cutives": {
                "Max Trades Gagnants": {
                    "Nombre": long_win_seq,
                    "Date D√©but": long_win_start,
                    "Date Fin": long_win_end
                },
                "Max Trades Perdants": {
                    "Nombre": long_lose_seq,
                    "Date D√©but": long_lose_start,
                    "Date Fin": long_lose_end
                }
            }
        },

        "Trades Shorts": {
            "Total": total_trades_short,
            "R√©ussis": num_winning_trades_short,
            "√âchou√©s": num_losing_trades_short,
            "Win Rate": win_rate_short,
            "PnL Moyen": expected_pnl_short,
            "Meilleur Trade": get_extreme_trade(short_trades, 'max'),
            "Pire Trade": get_extreme_trade(short_trades, 'min'),
            "S√©quences Cons√©cutives": {
                "Max Trades Gagnants": {
                    "Nombre": short_win_seq,
                    "Date D√©but": short_win_start,
                    "Date Fin": short_win_end
                },
                "Max Trades Perdants": {
                    "Nombre": short_lose_seq,
                    "Date D√©but": short_lose_start,
                    "Date Fin": short_lose_end
                }
            }
        },

        "Performance Globale": {
            "Win Rate Total": win_rate,
            "Gross Profit": gross_profit,
            "Gross Loss": gross_loss,
            "Net PnL": net_pnl,
            "Profit Factor": profit_factor,
            "PnL Moyen par Trade": expected_pnl_net,
            "Profit Moyen (Trades Gagnants)": avg_profit_per_win,
            "Perte Moyenne (Trades Perdants)": avg_loss_per_loss
        }
    }
def print_comparative_performance(metrics_before, metrics_after):
    """
    Affiche une comparaison d√©taill√©e et compl√®te des performances avant et apr√®s filtrage.

    Parameters:
    -----------
    metrics_before : dict
        M√©triques de performance avant filtrage
    metrics_after : dict
        M√©triques de performance apr√®s filtrage
    """

    def calculate_change(before, after):
        if isinstance(before, (int, float)) and isinstance(after, (int, float)):
            if before == 0:
                return "N/A" if after == 0 else "+‚àû" if after > 0 else "-‚àû"
            change = ((after - before) / abs(before)) * 100
            return f"{change:+.2f}%"
        return "N/A"

    print("\n‚ïê‚ïê‚ïê ANALYSE COMPARATIVE DES PERFORMANCES ‚ïê‚ïê‚ïê")

    print("\nüìä STATISTIQUES GLOBALES")
    print("‚ïê" * 75)
    print(f"{'M√©trique':<35} {'Avant':<15} {'Apr√®s':<15} {'Variation':<15}")
    print("‚îÄ" * 75)

    # Statistiques de base
    basic_metrics = {
        'Nombre total de trades': 'Total Trades',
        'Trades R√©ussis': 'Trades R√©ussis',
        'Trades √âchou√©s': 'Trades √âchou√©s'
    }

    for label, key in basic_metrics.items():
        before_val = metrics_before[key]
        after_val = metrics_after[key]
        print(f"{label:<35} {before_val:<15} {after_val:<15} {calculate_change(before_val, after_val):<15}")

    print("\nüìà PERFORMANCE GLOBALE")
    print("‚ïê" * 75)

    global_metrics = {
        'Win Rate Total (%)': 'Win Rate Total',
        'Gross Profit': 'Gross Profit',
        'Gross Loss': 'Gross Loss',
        'Net PnL': 'Net PnL',
        'Profit Factor': 'Profit Factor',
        'PnL Moyen par Trade': 'PnL Moyen par Trade',
        'Profit Moyen (Trades Gagnants)': 'Profit Moyen (Trades Gagnants)',
        'Perte Moyenne (Trades Perdants)': 'Perte Moyenne (Trades Perdants)'
    }

    for label, key in global_metrics.items():
        before_val = metrics_before['Performance Globale'][key]
        after_val = metrics_after['Performance Globale'][key]
        print(f"{label:<35} {before_val:15.2f} {after_val:15.2f} {calculate_change(before_val, after_val):<15}")

    # Analyse d√©taill√©e par direction
    directions = ['Longs', 'Shorts']
    for direction in directions:
        print(f"\nüìä ANALYSE DES TRADES {direction.upper()}")
        print("‚ïê" * 75)

        direction_metrics = {
            'Nombre total': 'Total',
            'Trades R√©ussis': 'R√©ussis',
            'Trades √âchou√©s': '√âchou√©s',
            'Win Rate (%)': 'Win Rate',
            'PnL Moyen': 'PnL Moyen'
        }

        for label, key in direction_metrics.items():
            before_val = metrics_before[f'Trades {direction}'][key]
            after_val = metrics_after[f'Trades {direction}'][key]
            print(f"{label:<35} {before_val:15.2f} {after_val:15.2f} {calculate_change(before_val, after_val):<15}")

        # Meilleurs et pires trades avec leurs dates
        print(f"\nüéØ TRADES EXTR√äMES {direction.upper()}")
        print("‚îÄ" * 75)

        # Meilleur trade
        best_before = metrics_before[f'Trades {direction}']['Meilleur Trade']
        best_after = metrics_after[f'Trades {direction}']['Meilleur Trade']
        print(f"Meilleur trade avant: {best_before['PnL']:.2f} (Date: {best_before['Date']})")
        print(f"Meilleur trade apr√®s: {best_after['PnL']:.2f} (Date: {best_after['Date']})")

        # Pire trade
        worst_before = metrics_before[f'Trades {direction}']['Pire Trade']
        worst_after = metrics_after[f'Trades {direction}']['Pire Trade']
        print(f"Pire trade avant: {worst_before['PnL']:.2f} (Date: {worst_before['Date']})")
        print(f"Pire trade apr√®s: {worst_after['PnL']:.2f} (Date: {worst_after['Date']})")

    # R√©sum√© de l'impact du filtrage
    print("\nüìë R√âSUM√â DE L'IMPACT DU FILTRAGE")
    print("‚ïê" * 75)
    trades_removed = metrics_before['Total Trades'] - metrics_after['Total Trades']
    trades_removed_pct = (trades_removed / metrics_before['Total Trades']) * 100

    print(f"Trades filtr√©s: {trades_removed} ({trades_removed_pct:.2f}% du total)")

    wr_impact = metrics_after['Performance Globale']['Win Rate Total'] - metrics_before['Performance Globale'][
        'Win Rate Total']
    print(f"Impact sur le Win Rate: {wr_impact:+.2f}%")

    pnl_impact = metrics_after['Performance Globale']['Net PnL'] - metrics_before['Performance Globale']['Net PnL']
    print(f"Impact sur le Net PnL: {pnl_impact:+.2f}")

    pf_impact = metrics_after['Performance Globale']['Profit Factor'] - metrics_before['Performance Globale'][
        'Profit Factor']
    print(f"Impact sur le Profit Factor: {pf_impact:+.2f}")

    # Ajouter la section des s√©quences juste avant le r√©sum√© de l'impact
    for direction in ['Longs', 'Shorts']:
        print(f"\nüìä S√âQUENCES CONS√âCUTIVES {direction.upper()}")
        print("‚ïê" * 75)

        for period, metrics in [("Avant", metrics_before), ("Apr√®s", metrics_after)]:
            sequences = metrics[f"Trades {direction}"]["S√©quences Cons√©cutives"]
            print(f"\n{period}:")

            # Affichage des trades gagnants cons√©cutifs
            win_seq = sequences["Max Trades Gagnants"]
            print(f"Trades gagnants cons√©cutifs maximum : {win_seq['Nombre']}")
            if win_seq['Nombre'] > 0:
                print(f"  P√©riode : du {win_seq['Date D√©but']} au {win_seq['Date Fin']}")

            # Affichage des trades perdants cons√©cutifs
            lose_seq = sequences["Max Trades Perdants"]
            print(f"Trades perdants cons√©cutifs maximum : {lose_seq['Nombre']}")
            if lose_seq['Nombre'] > 0:
                print(f"  P√©riode : du {lose_seq['Date D√©but']} au {lose_seq['Date Fin']}")
            print()

def calculate_statistical_power(X, y, feature_list=None,
                                alpha=0.05, target_power=0.8, n_simulations=20000,
                                sample_fraction=0.8, verbose=True,
                                method_powerAnaly='both'):
    """
    Calcule la puissance statistique analytique et/ou par simulation Monte Carlo.

    Parameters:
    -----------
    X : pandas.DataFrame
        DataFrame contenant uniquement les features √† analyser
    y : pandas.Series
        S√©rie contenant la variable cible binaire (0/1)
    feature_list : list, optional
        Liste des noms de features √† analyser. Si None, utilise toutes les colonnes de X
    alpha : float, default=0.05
        Seuil de significativit√©
    target_power : float, default=0.8
        Puissance statistique cible
    n_simulations : int, default=10000
        Nombre de simulations Monte Carlo
    sample_fraction : float, default=0.8
        Fraction de l'√©chantillon √† utiliser dans chaque simulation
    verbose : bool, default=True
        Afficher les messages d'avancement
    method : str, default='both'
        M√©thode de calcul de la puissance √† utiliser. Options:
        - 'both': calcule la puissance analytique et Monte Carlo
        - 'analytical': calcule uniquement la puissance analytique
        - 'montecarlo': calcule uniquement la puissance par simulation Monte Carlo

    Returns:
    --------
    pandas.DataFrame
        DataFrame contenant les r√©sultats de l'analyse de puissance
    """
    # V√©rifier que la m√©thode est valide
    valid_methods = ['both', 'analytical', 'montecarlo']
    if method_powerAnaly not in valid_methods:
        raise ValueError(f"La m√©thode '{method_powerAnaly}' n'est pas valide. Options: {', '.join(valid_methods)}")

    if feature_list is None:
        feature_list = X.columns.tolist()
    else:
        # S'assurer que toutes les features demand√©es existent dans X
        feature_list = [f for f in feature_list if f in X.columns]

    # Filtrer les colonnes constantes (n'ayant qu'une seule valeur unique)
    constant_features = [col for col in feature_list if X[col].nunique() <= 1]
    if constant_features and verbose:
        print(f"‚ö†Ô∏è {len(constant_features)} features constantes retir√©es: {constant_features}")

    feature_list = [f for f in feature_list if f not in constant_features]

    results = []
    power_analysis = TTestIndPower()
    total_features = len(feature_list)

    for i, feature in enumerate(feature_list):
        if verbose and i % max(1, total_features // 10) == 0:
            print(f"Traitement: {i + 1}/{total_features} features ({((i + 1) / total_features) * 100:.1f}%)")

        # Pr√©paration des donn√©es pour cette feature
        X_feature = X[feature].copy()

        # Filtrer les valeurs NaN
        mask = X_feature.notna() & y.notna()
        X_filtered = X_feature[mask].values
        y_filtered = y[mask].values

        # S√©parer les groupes
        group0 = X_filtered[y_filtered == 0]
        group1 = X_filtered[y_filtered == 1]

        # V√©rifier que les deux groupes ont suffisamment de donn√©es
        if len(group0) <= 1 or len(group1) <= 1:
            if verbose:
                print(f"‚ö†Ô∏è Skipping {feature}: Not enough data in both groups")
            continue

        # Calcul de l'effet de taille (Cohen's d)
        #mean_diff = np.mean(group1) - np.mean(group0)
        mean_diff = np.median(group1) - np.median(group0)

        pooled_std = np.sqrt(((len(group0) - 1) * np.std(group0, ddof=1) ** 2 +
                              (len(group1) - 1) * np.std(group1, ddof=1) ** 2) /
                             (len(group0) + len(group1) - 2))

        if pooled_std == 0:
            if verbose:
                print(f"‚ö†Ô∏è Skipping {feature}: Zero variance in data")
            continue

        effect_size = mean_diff / pooled_std

        # Test statistique de base (t-test de Welch)
        t_stat, p_value = stats.ttest_ind(group0, group1, equal_var=False)

        # Initialiser les valeurs par d√©faut
        power_analytical = None
        power_monte_carlo = None
        se_monte_carlo = None
        mc_ci_lower = None
        mc_ci_upper = None
        required_n = np.nan

        # M√©thode 1: Puissance Analytique (si demand√©e)
        if method_powerAnaly in ['both', 'analytical']:
            power_analytical = power_analysis.power(
                effect_size=abs(effect_size),  # Utiliser valeur absolue
                nobs1=len(group0),
                alpha=alpha,
                ratio=len(group1) / len(group0)
            )

            # Calcul de la taille d'√©chantillon requise
            try:
                required_n = power_analysis.solve_power(
                    effect_size=abs(effect_size),
                    power=target_power,
                    alpha=alpha,
                    ratio=len(group1) / len(group0)
                )
            except (ValueError, RuntimeError):
                required_n = np.nan

        # M√©thode 2: Puissance Monte Carlo par simulation (si demand√©e)
        if method_powerAnaly in ['both', 'montecarlo']:
            significant_count = 0
            for _ in range(n_simulations):
                # √âchantillonnage al√©atoire
                if len(group0) > 1 and len(group1) > 1:
                    sample0_size = max(2, int(len(group0) * sample_fraction))
                    sample1_size = max(2, int(len(group1) * sample_fraction))

                    sample0 = np.random.choice(group0, size=sample0_size, replace=True)
                    sample1 = np.random.choice(group1, size=sample1_size, replace=True)

                    # Test t sur l'√©chantillon
                    _, p_sim = stats.ttest_ind(sample0, sample1, equal_var=False)
                    if p_sim < alpha:
                        significant_count += 1

            power_monte_carlo = significant_count / n_simulations

            # Calcul de l'erreur standard de la puissance Monte Carlo
            se_monte_carlo = np.sqrt(power_monte_carlo * (1 - power_monte_carlo) / n_simulations)
            mc_ci_lower = max(0, power_monte_carlo - 1.96 * se_monte_carlo)
            mc_ci_upper = min(1, power_monte_carlo + 1.96 * se_monte_carlo)

        # D√©terminer quelle puissance utiliser pour la colonne Power_Sufficient
        if method_powerAnaly == 'both':
            power_for_sufficiency = power_monte_carlo
        elif method_powerAnaly == 'analytical':
            power_for_sufficiency = power_analytical
        else:  # montecarlo
            power_for_sufficiency = power_monte_carlo

        # Calcul de la diff√©rence entre les m√©thodes
        delta_power = None
        if method_powerAnaly == 'both' and power_analytical is not None and power_monte_carlo is not None:
            delta_power = abs(power_analytical - power_monte_carlo)

        # Ajouter les r√©sultats
        result_row = {
            'Feature': feature,
            'Sample_Size': len(X_filtered),
            'Group0_Size': len(group0),
            'Group1_Size': len(group1),
            'Effect_Size': effect_size,
            'P_Value': p_value,
            'Required_N': np.ceil(required_n) if not np.isnan(required_n) else np.nan,
            'Power_Sufficient': power_for_sufficiency is not None and power_for_sufficiency >= target_power
        }

        # Ajouter les colonnes sp√©cifiques √† la m√©thode analytique
        if method_powerAnaly in ['both', 'analytical']:
            result_row['Power_Analytical'] = power_analytical

        # Ajouter les colonnes sp√©cifiques √† la m√©thode Monte Carlo
        if method_powerAnaly in ['both', 'montecarlo']:
            result_row['Power_MonteCarlo'] = power_monte_carlo
            result_row['MC_StdError'] = se_monte_carlo
            result_row['MC_CI_Lower'] = mc_ci_lower
            result_row['MC_CI_Upper'] = mc_ci_upper

        # Ajouter la diff√©rence entre les m√©thodes si les deux sont calcul√©es
        if delta_power is not None:
            result_row['Delta_Power'] = delta_power

        results.append(result_row)

    # Cr√©er le DataFrame des r√©sultats
    results_df = pd.DataFrame(results)

    if not results_df.empty:
        # # Trier par la puissance appropri√©e selon la m√©thode choisie
        # if method_powerAnaly == 'both' or method_powerAnaly == 'montecarlo':
        #     sort_column = 'Power_MonteCarlo'
        # else:  # analytical
        #     sort_column = 'Power_Analytical'

        # if sort_column in results_df.columns:
        #     results_df = results_df.sort_values(sort_column, ascending=False)

        if verbose:
            print(f"\nAnalyse termin√©e. {len(results_df)} features analys√©es.")

            # Statistiques sur les diff√©rences de puissance si applicable
            if 'Delta_Power' in results_df.columns:
                mean_delta = results_df['Delta_Power'].mean()
                max_delta = results_df['Delta_Power'].max()
                print(f"Diff√©rence moyenne entre les m√©thodes: {mean_delta:.4f}")
                print(f"Diff√©rence maximale entre les m√©thodes: {max_delta:.4f}")

            # Features avec puissance suffisante
            sufficient_power = results_df[results_df['Power_Sufficient']].shape[0]
            print(
                f"Features avec puissance suffisante (>= {target_power}): {sufficient_power} sur {len(results_df)}")
    else:
        if verbose:
            print("Aucun r√©sultat obtenu. V√©rifiez vos donn√©es et les param√®tres.")

    return results_df


import numpy as np
import pandas as pd
from scipy import stats
from statsmodels.stats.power import TTestIndPower
from joblib import Parallel, delayed

from scipy.stats import ttest_ind, mannwhitneyu, normaltest, levene
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, shapiro

def run_single_simulation_auto(group0, group1, sample_fraction, alpha):
    """
    Simulation Monte Carlo qui choisit automatiquement entre Test t et Mann-Whitney.

    """
    sample0_size = max(2, int(len(group0) * sample_fraction))
    sample1_size = max(2, int(len(group1) * sample_fraction))

    sample0 = np.random.choice(group0, size=sample0_size, replace=True)
    sample1 = np.random.choice(group1, size=sample1_size, replace=True)

    # V√©rifier la normalit√© uniquement si les √©chantillons sont suffisants
    if len(sample0) > 20 and len(sample1) > 20:
        norm_test_0 = normaltest(sample0).pvalue
        norm_test_1 = normaltest(sample1).pvalue
        var_test_p = levene(sample0, sample1).pvalue  # Test de variance

        if norm_test_0 > 0.05 and norm_test_1 > 0.05:  # Si les 2 sont normaux
            _, p_sim = ttest_ind(sample0, sample1, equal_var=(var_test_p > 0.05))
        else:
            _, p_sim = mannwhitneyu(sample0, sample1, alternative='two-sided')
    else:
        _, p_sim = mannwhitneyu(sample0, sample1, alternative='two-sided')  # Cas petit √©chantillon

    return p_sim < alpha


from scipy.stats import ttest_ind, mannwhitneyu, normaltest, levene
from statsmodels.stats.power import TTestIndPower

def calculate_statistical_power_job(X, y, feature_list=None,
                                    alpha=0.05, target_power=0.8, n_simulations_monte=20000,
                                    sample_fraction=0.8, verbose=True,
                                    method_powerAnaly='both', n_jobs=-1):
    """
    Calcule la puissance statistique analytique et/ou par simulation Monte Carlo.
    """

    valid_methods = ['both', 'analytical', 'montecarlo']
    if method_powerAnaly not in valid_methods:
        raise ValueError(f"La m√©thode '{method_powerAnaly}' n'est pas valide. Options: {', '.join(valid_methods)}")

    if feature_list is None:
        feature_list = X.columns.tolist()
    else:
        feature_list = [f for f in feature_list if f in X.columns]

    constant_features = [col for col in feature_list if X[col].nunique() <= 1]
    if constant_features and verbose:
        print(f"‚ö†Ô∏è {len(constant_features)} features constantes retir√©es: {constant_features}")

    feature_list = [f for f in feature_list if f not in constant_features]

    results = []
    power_analysis = TTestIndPower()
    total_features = len(feature_list)

    for i, feature in enumerate(feature_list):
        if verbose and i % max(1, total_features // 10) == 0:
            print(f"Traitement: {i + 1}/{total_features} features ({((i + 1) / total_features) * 100:.1f}%)")

        X_feature = X[feature].copy()
        mask = X_feature.notna() & y.notna()
        X_filtered = X_feature[mask].values
        y_filtered = y[mask].values

        group0 = X_filtered[y_filtered == 0]
        group1 = X_filtered[y_filtered == 1]

        if len(group0) <= 1 or len(group1) <= 1:
            continue

        mean_diff = np.mean(group1) - np.mean(group0)
        pooled_std = np.sqrt(((len(group0) - 1) * np.std(group0, ddof=1) ** 2 +
                              (len(group1) - 1) * np.std(group1, ddof=1) ** 2) /
                             (len(group0) + len(group1) - 2))

        if pooled_std == 0:
            continue

        effect_size = mean_diff / pooled_std

        # **üöÄ Nouvelle logique : V√©rification de la normalit√© et des variances**
        if len(group0) > 20 and len(group1) > 20:
            norm_test_0 = normaltest(group0).pvalue
            norm_test_1 = normaltest(group1).pvalue
            var_test_p = levene(group0, group1).pvalue  # V√©rification de la variance

            if norm_test_0 > 0.05 and norm_test_1 > 0.05:
                print("# **Les deux distributions sont normales**");
                t_stat, p_value = ttest_ind(group0, group1, equal_var=(var_test_p > 0.05))
                power_analytical = power_analysis.power(
                    effect_size=abs(effect_size),
                    nobs1=len(group0),
                    alpha=alpha,
                    ratio=len(group1) / len(group0)
                )
            else:
                # **Au moins une distribution est non normale ‚Üí Mann-Whitney U**
                print("# **Au moins une distribution est non normale ‚Üí Mann-Whitney U****");

                t_stat, p_value = mannwhitneyu(group0, group1, alternative='two-sided')
                power_analytical = None  # ‚ö†Ô∏è Pas de formule analytique pour Mann-Whitney
        else:
            # **Petit √©chantillon ‚Üí Mann-Whitney par d√©faut**
            print(" # **Petit √©chantillon ‚Üí Mann-Whitney par d√©faut**");
            t_stat, p_value = mannwhitneyu(group0, group1, alternative='two-sided')
            power_analytical = None  # ‚ö†Ô∏è Pas de formule analytique pour Mann-Whitney

        required_n = None
        if power_analytical is not None:
            try:
                required_n = power_analysis.solve_power(
                    effect_size=abs(effect_size),
                    power=target_power,
                    alpha=alpha,
                    ratio=len(group1) / len(group0)
                )
            except (ValueError, RuntimeError):
                required_n = np.nan

        # **üöÄ Monte Carlo Simulation**
        power_monte_carlo = None
        if method_powerAnaly in ['both', 'montecarlo']:
            results_parallel = Parallel(n_jobs=n_jobs)(
                delayed(run_single_simulation_auto)(group0, group1, sample_fraction, alpha)
                for _ in range(n_simulations_monte)
            )
            power_monte_carlo = np.mean(results_parallel)

        # **üöÄ Stockage des r√©sultats**
        result_row = {
            'Feature': feature,
            'Sample_Size': len(X_filtered),
            'Group0_Size': len(group0),
            'Group1_Size': len(group1),
            'Effect_Size': effect_size,
            'P_Value': p_value,
            'Required_N': np.ceil(required_n) if required_n is not None else np.nan,
            'Power_Analytical': power_analytical,
            'Power_MonteCarlo': power_monte_carlo,
        }

        results.append(result_row)

    return pd.DataFrame(results)




def create_full_dataframe_with_filtered_pnl(df_init_features, df_filtered):
        # Cr√©ation d'une copie du DataFrame initial
        df_full_afterFiltering = df_init_features.copy()

        # Cr√©ation d'une colonne PnlAfterFiltering initialis√©e √† 0.0 (comme un float)
        # Cela garantit que la colonne sera cr√©√©e avec un type compatible avec vos valeurs de PnL
        df_full_afterFiltering['PnlAfterFiltering'] = 0.0

        # Identification des lignes qui ont pass√© le filtrage
        filtered_indices = df_filtered.index

        # Pour ces lignes, on attribue le PnL original √† la nouvelle colonne
        # Conversion explicite en float pour s'assurer de la compatibilit√©
        df_full_afterFiltering.loc[filtered_indices, 'PnlAfterFiltering'] = df_filtered['trade_pnl'].astype(float)

        return df_full_afterFiltering

import pandas as pd

def preprocess_sessions_with_date(df):
    """
    Pr√©traite les donn√©es de sessions en:
    1. Cr√©ant une colonne 'session_id' √† partir des marqueurs SessionStartEnd
    2. Ajoutant une colonne 'session_date' qui contient la date du dernier enregistrement de chaque session

    Args:
        df (pd.DataFrame): DataFrame avec la colonne SessionStartEnd (10=d√©but, 20=fin) et 'date'

    Returns:
        pd.DataFrame: DataFrame avec les nouvelles colonnes 'session_id' et 'session_date'
    """
    print_notification("Cr√©ation des identifiants de session et dates de session")

    # Copier le DataFrame pour √©viter de modifier l'original
    df_copy = df.copy()

    # G√©n√©rer les IDs de session en cumulant les d√©buts de session
    df_copy['session_id'] = (df_copy['SessionStartEnd'] == 10).cumsum()

    # Identifier les sessions valides (celles qui ont un d√©but et une fin)
    session_valid = df_copy.groupby('session_id')['SessionStartEnd'].transform(lambda x: (10 in x.values) and (20 in x.values))

    # Supprimer les sessions incompl√®tes (sans fin)
    df_copy.loc[~session_valid, 'session_id'] = 0

    # Assigner la derni√®re date de chaque session
    df_copy['session_date'] = df_copy.groupby('session_id')['date'].transform('last')

    # Affichage des statistiques
    nb_sessions = df_copy['session_id'].nunique() - (0 in df_copy['session_id'].unique())
    print(f"‚úì {nb_sessions} sessions valides identifi√©es")

    return df_copy



import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd
import numpy as np
from datetime import datetime

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd
import numpy as np
import matplotlib.gridspec as gridspec  # Ajout de cette importation


def plot_trading_performance(df):
    """
    Cr√©e une visualisation √† quatre niveaux de la performance de trading :
    1. PnL cumul√© sur toutes les sessions avec statistiques d√©taill√©es
    2. Drawdown cumul√©
    3. PnL journalier par session
    4. Profil de performance intra-journali√®re type sur une session de 23h (de 22h √† 21h le lendemain)

    Args:
        df: DataFrame contenant 'session_date', 'PnlAfterFiltering', 'session_id' et 'deltaTimestampOpening'
    """
    # Pr√©paration des donn√©es
    # S'assurer que session_date est au format datetime
    if not pd.api.types.is_datetime64_any_dtype(df['session_date']):
        df['session_date'] = pd.to_datetime(df['session_date'])

    # Regrouper par date de session et sommer le PnL
    daily_pnl = df.groupby('session_date')['PnlAfterFiltering'].sum().reset_index()
    daily_pnl = daily_pnl.sort_values('session_date')

    # Calculer le PnL cumul√©
    daily_pnl['cumulative_pnl'] = daily_pnl['PnlAfterFiltering'].cumsum()

    # Calculer le drawdown
    daily_pnl['peak'] = daily_pnl['cumulative_pnl'].cummax()
    daily_pnl['drawdown'] = daily_pnl['peak'] - daily_pnl['cumulative_pnl']

    # Identifier les jours avec PnL max et min
    max_pnl_idx = daily_pnl['PnlAfterFiltering'].idxmax()
    min_pnl_idx = daily_pnl['PnlAfterFiltering'].idxmin()
    max_pnl_day = daily_pnl.loc[max_pnl_idx] if max_pnl_idx is not None else None
    min_pnl_day = daily_pnl.loc[min_pnl_idx] if min_pnl_idx is not None else None

    # Calculer le drawdown maximum
    max_drawdown = daily_pnl['drawdown'].max()
    max_dd_idx = daily_pnl['drawdown'].idxmax()
    max_dd_date = daily_pnl.loc[max_dd_idx, 'session_date'] if max_dd_idx is not None else None

    # Calculer les statistiques de performance globales
    # Identifier les trades gagnants et perdants
    winning_trades = df[df['PnlAfterFiltering'] > 0]
    losing_trades = df[df['PnlAfterFiltering'] < 0]

    # Calculer les m√©triques de performance
    total_trades = len(df[df['PnlAfterFiltering'] != 0])
    winning_trades_count = len(winning_trades)
    losing_trades_count = len(losing_trades)
    winrate = winning_trades_count / total_trades * 100 if total_trades > 0 else 0

    total_profit = winning_trades['PnlAfterFiltering'].sum()
    total_loss = abs(losing_trades['PnlAfterFiltering'].sum())
    profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')

    # Calculer l'expected PnL par trade
    # Filtrer pour n'inclure que les trades effectu√©s
    executed_trades = df[df['PnlAfterFiltering'] != 0]
    expected_pnl = executed_trades['PnlAfterFiltering'].mean() if len(executed_trades) > 0 else 0

    # Fonction d'aide pour formater les dates de mani√®re s√©curis√©e
    def format_date(date_obj):
        if pd.isna(date_obj):
            return "Date inconnue"
        try:
            # Pour pandas Timestamp
            if hasattr(date_obj, 'strftime'):
                return date_obj.strftime('%Y-%m-%d')
            # Pour les dates en string ou autres formats
            return str(date_obj).split(' ')[0]
        except:
            return str(date_obj)

    # Cr√©ation de la figure avec quatre sous-graphiques
    fig, axes = plt.subplots(4, 1, figsize=(15, 18), sharex=False,
                             gridspec_kw={'height_ratios': [2, 1, 1.5, 2.5]})

    # 1. Graphique du PnL cumul√©
    axes[0].plot(daily_pnl['session_date'], daily_pnl['cumulative_pnl'], 'b-', linewidth=2)
    axes[0].set_title('PnL Cumul√©', fontsize=14)
    axes[0].grid(True, alpha=0.3)
    axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)
    axes[0].set_ylabel('PnL ($)', fontsize=12)

    # Annotation finale avec les statistiques d√©taill√©es
    if len(daily_pnl) > 0:
        final_pnl = daily_pnl['cumulative_pnl'].iloc[-1]

        # Cr√©er un texte avec toutes les statistiques
        stats_text = f"PnL Final: ${final_pnl:.2f}\n"
        stats_text += f"Trades: {total_trades} (Gagnants: {winning_trades_count}, Perdants: {losing_trades_count})\n"
        stats_text += f"Winrate: {winrate:.2f}%\n"
        stats_text += f"Profit: ${total_profit:.2f}, Pertes: ${total_loss:.2f}\n"
        stats_text += f"Profit Factor: {profit_factor:.2f}\n"
        stats_text += f"Expected PnL: ${expected_pnl:.2f}"

        # Obtenir la transformation des coordonn√©es entre les axes et la figure
        bbox = axes[0].get_position()

        # Placer l'annotation dans l'espace de la figure, juste √† droite des axes
        # Note: l'axe des x va de 0 √† 1 dans l'espace de la figure
        # Obtenir la transformation des coordonn√©es entre les axes et la figure
        bbox = axes[0].get_position()

        # Placer l'annotation dans l'espace de la figure, un peu moins √† droite
        fig.text(
            bbox.x1 - 0.1,  # Position x: l√©g√®rement √† gauche de la fin des axes
            bbox.y0 + bbox.height / 2,  # Position y: milieu vertical des axes
            stats_text,
            verticalalignment='center',
            horizontalalignment='left',
            fontsize=10,
            bbox=dict(boxstyle="round,pad=0.5", facecolor='white', alpha=0.8, edgecolor='gray')
        )

    # 2. Graphique du drawdown
    axes[1].fill_between(daily_pnl['session_date'], 0, daily_pnl['drawdown'], color='r', alpha=0.3)
    axes[1].plot(daily_pnl['session_date'], daily_pnl['drawdown'], 'r-', linewidth=1)
    axes[1].set_title('Drawdown', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylabel('Drawdown ($)', fontsize=12)

    # Annotation du drawdown maximum
    if max_dd_date is not None:
        formatted_date = format_date(max_dd_date)
        axes[1].annotate(f'Max Drawdown: ${max_drawdown:.2f} ({formatted_date})',
                         xy=(max_dd_date, max_drawdown),
                         xytext=(10, -20), textcoords='offset points',
                         arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"),
                         fontweight='bold', color='darkred')

    # 3. Graphique du PnL journalier
    bar_colors = ['g' if x >= 0 else 'r' for x in daily_pnl['PnlAfterFiltering']]
    axes[2].bar(daily_pnl['session_date'], daily_pnl['PnlAfterFiltering'], color=bar_colors, alpha=0.7)
    axes[2].set_title('PnL Journalier', fontsize=14)
    axes[2].grid(True, alpha=0.3)
    axes[2].set_ylabel('PnL ($)', fontsize=12)
    #axes[2].set_xlabel('Date', fontsize=12)

    # Formater l'axe des x pour qu'il affiche les dates correctement
    axes[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # D√©terminer la fr√©quence appropri√©e des ticks en fonction du nombre de jours
    if len(daily_pnl) > 45:
        # Si beaucoup de donn√©es, montrer moins de ticks pour √©viter l'encombrement
        axes[2].xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=0))  # Lundi
    else:
        # Si peu de donn√©es, montrer toutes les dates
        axes[2].xaxis.set_major_locator(mdates.DayLocator())

    # Ajuster la taille et l'angle des √©tiquettes de date
    for label in axes[2].get_xticklabels():
        label.set_fontsize(8)  # Taille de police 8
        label.set_rotation(45)  # Inclinaison √† 45 degr√©s
        label.set_ha('right')  # Alignement √† droite pour √©viter le chevauchement
        label.set_rotation_mode('anchor')  # Mode d'ancrage pour une meilleure rotation

    # Annotations pour le PnL journalier max et min
    if max_pnl_day is not None:
        axes[2].annotate(f'Max: ${max_pnl_day["PnlAfterFiltering"]:.2f}',
                         xy=(max_pnl_day['session_date'], max_pnl_day['PnlAfterFiltering']),
                         xytext=(0, 20), textcoords='offset points',
                         arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"),
                         fontweight='bold', color='darkgreen')

    if min_pnl_day is not None:
        axes[2].annotate(f'Min: ${min_pnl_day["PnlAfterFiltering"]:.2f}',
                         xy=(min_pnl_day['session_date'], min_pnl_day['PnlAfterFiltering']),
                         xytext=(0, -20), textcoords='offset points',
                         arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"),
                         fontweight='bold', color='darkred')

    # 4. Graphique du profil intra-journalier (tranches de 30 minutes)
    # 4. Graphique du profil intra-journalier (tranches de 30 minutes) - DIVIS√â EN DEUX
    if 'deltaTimestampOpening' in df.columns:
        # Convertir deltaTimestampOpening en num√©rique si ce n'est pas d√©j√† le cas
        if not pd.api.types.is_numeric_dtype(df['deltaTimestampOpening']):
            df['deltaTimestampOpening'] = pd.to_numeric(df['deltaTimestampOpening'], errors='coerce')

        # Limiter aux donn√©es pertinentes (dans la plage de 0 √† 1380 minutes)
        df_valid = df[(df['deltaTimestampOpening'] >= 0) & (df['deltaTimestampOpening'] <= 1380)]

        # Arrondir √† la tranche de 30 minutes la plus proche
        df_valid['time_bin'] = (df_valid['deltaTimestampOpening'] // 30) * 30

        # Cr√©er une sous-figure avec deux graphiques c√¥te √† c√¥te pour la 4√®me ligne
        gs = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=plt.GridSpec(4, 1)[3], width_ratios=[1, 1], wspace=0.3)

        # 4.1 GAUCHE - PnL moyen par intervalle de 30 minutes (graphique existant)
        ax_left = fig.add_subplot(gs[0])

        # Calculer le PnL moyen par intervalle de 30 minutes pour toutes les sessions
        intraday_profile = df_valid.groupby('time_bin')['PnlAfterFiltering'].agg(['sum', 'mean', 'count']).reset_index()
        intraday_profile = intraday_profile.sort_values('time_bin')

        # Fonction pour convertir les minutes depuis l'ouverture (22h) en format d'heure lisible
        def format_trading_time(minutes):
            total_hours = 22 + minutes // 60
            day_marker = "" if total_hours < 24 else "+1j"  # Indicateur de jour suivant
            hours = total_hours % 24
            mins = minutes % 60
            return f"{hours:02d}:{mins:02d}{day_marker}"

        # Cr√©er un tableau complet de toutes les tranches de 30 minutes sur 23h
        all_bins = np.arange(0, 1380 + 30, 30)  # De 0 √† 1380 minutes (23h), par tranches de 30 min
        all_times = pd.DataFrame({'time_bin': all_bins})

        # Fusionner avec les donn√©es r√©elles
        complete_profile = all_times.merge(intraday_profile, on='time_bin', how='left').fillna(0)
        complete_profile = complete_profile.sort_values('time_bin')

        # Calculer le PnL cumulatif moyen sur la journ√©e
        complete_profile['cumulative_mean'] = complete_profile['mean'].cumsum()

        # Tracer le graphique des barres de PnL par tranche de 30 minutes
        bar_colors = ['g' if x >= 0 else 'r' for x in complete_profile['mean']]
        ax_left.bar(complete_profile['time_bin'], complete_profile['mean'],
                    width=25, alpha=0.7, color=bar_colors)

        # Ajouter la courbe du PnL cumulatif
        ax_twin = ax_left.twinx()  # Axe secondaire pour le PnL cumulatif
        ax_twin.plot(complete_profile['time_bin'], complete_profile['cumulative_mean'],
                     'b-', linewidth=2, alpha=0.8)
        ax_twin.set_ylabel('PnL Cumulatif Moyen ($)', fontsize=10)

        # Marquer le passage √† minuit avec une ligne verticale
        midnight = 120  # 120 minutes apr√®s 22h = minuit
        ax_left.axvline(x=midnight, color='k', linestyle='--', alpha=0.5)
        ax_left.text(midnight + 5, ax_left.get_ylim()[1] * 0.9, 'Minuit',
                     fontsize=9, rotation=90, va='top')

        # Configurer l'axe X avec des intervalles de temps lisibles
        # Afficher une √©tiquette toutes les heures (pour √©conomiser de l'espace)
        hour_ticks = np.arange(0, 1380 + 180, 180)  # Toutes les 3 heures
        ax_left.set_xticks(hour_ticks)
        ax_left.set_xticklabels([format_trading_time(t) for t in hour_ticks],
                                rotation=45, fontsize=8, ha='right')

        # Limiter l'affichage explicitement de 0 √† 1380 minutes (23 heures)
        ax_left.set_xlim(0, 1380)

        # Ajouter le titre et les √©tiquettes
        ax_left.set_title('PnL Moyen par Tranche de 30min', fontsize=12)
        ax_left.set_xlabel('Heure de la journ√©e de trading', fontsize=10)
        ax_left.set_ylabel('PnL Moyen par 30min ($)', fontsize=10)
        ax_left.grid(True, alpha=0.3)

        # 4.2 DROITE - Volume de trades et winrate par intervalle de 30 minutes
        ax_right = fig.add_subplot(gs[1])

        # Ne consid√©rer que les trades qui ont √©t√© ex√©cut√©s (PnL non nul)
        executed_trades = df_valid[df_valid['PnlAfterFiltering'] != 0]

        # Calculer les m√©triques par tranche de 30 minutes
        volume_by_time = executed_trades.groupby('time_bin').apply(
            lambda x: pd.Series({
                'R√©ussis': sum(x['PnlAfterFiltering'] > 0),
                '√âchou√©s': sum(x['PnlAfterFiltering'] < 0),
                'Total': len(x)
            })
        ).reset_index()

        # Calculer le winrate
        volume_by_time['Winrate'] = (
                volume_by_time['R√©ussis'] / volume_by_time['Total'] * 100
        ).replace([np.inf, -np.inf, np.nan], 0)

        # Fusionner avec la grille compl√®te (all_times) pour avoir toutes les tranches
        complete_volume = all_times.merge(volume_by_time, on='time_bin', how='left').fillna(0)

        # Cr√©er le graphique √† barres empil√©es pour le volume (sans label pour ne pas afficher de l√©gende)
        ax_right.bar(
            complete_volume['time_bin'],
            complete_volume['√âchou√©s'],
            width=25, alpha=0.7, color='r'
        )
        ax_right.bar(
            complete_volume['time_bin'],
            complete_volume['R√©ussis'],
            width=25, alpha=0.7, color='g',
            bottom=complete_volume['√âchou√©s']
        )

        # Ajouter le winrate au-dessus de chaque barre (une seule fois)
        for i, row in complete_volume.iterrows():
            if row['Total'] > 0:  # n'afficher que si au moins un trade
                # Position du texte juste au-dessus de la barre empil√©e
                bar_top = row['√âchou√©s'] + row['R√©ussis']
                ax_right.text(
                    row['time_bin'],
                    bar_top + 1,  # petit d√©calage vertical pour √™tre lisible
                    f"{row['Winrate']:.0f}%",  # Winrate sans d√©cimale
                    ha='center', va='bottom',
                    fontsize=8, rotation=90, color='black'
                )

        # Configurer l'axe X avec les m√™mes intervalles que le graphique de gauche
        ax_right.set_xticks(hour_ticks)
        ax_right.set_xticklabels(
            [format_trading_time(t) for t in hour_ticks],
            rotation=45, fontsize=8, ha='right'
        )

        # Marquer le passage √† minuit
        ax_right.axvline(x=midnight, color='k', linestyle='--', alpha=0.5)

        # Limiter l'affichage
        ax_right.set_xlim(0, 1380)

        # Ajouter titre/labels
        ax_right.set_title('Volume de Trades et Winrate par Tranche de 30min', fontsize=12)
        ax_right.set_xlabel('Heure de la journ√©e de trading', fontsize=10)
        ax_right.set_ylabel('Nombre de Trades', fontsize=10)
        ax_right.grid(True, alpha=0.3)

        # Enlever la l√©gende (pas de ax_right.legend(...) et pas de label=)
        # Ajouter une annotation sur le nombre de sessions
        session_count = df_valid['session_id'].nunique()
        ax_right.annotate(
            f"Bas√© sur {session_count} sessions",
            xy=(0.98, 0.02), xycoords='axes fraction',
            ha='right', va='bottom', fontsize=8,
            bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="gray", alpha=0.7)
        )

    # Ajuster l'espacement pour accommoder le 4√®me graphique
    plt.tight_layout()
    fig.subplots_adjust(hspace=0.4, bottom=0.1)

    # Ajouter un titre global ajust√© avec plus d'espace
    fig.suptitle('Analyse de Performance de Trading', fontsize=16, y=0.95)

    # Imprimer quelques statistiques cl√©s
    if len(daily_pnl) > 0:
        print(
            f"P√©riode d'analyse: {format_date(daily_pnl['session_date'].min())} √† {format_date(daily_pnl['session_date'].max())}")
        final_pnl = daily_pnl['cumulative_pnl'].iloc[-1]
        print(f"PnL cumul√© final: ${final_pnl:.2f}")
        print(f"Drawdown maximum: ${max_drawdown:.2f} le {format_date(max_dd_date)}")

        if max_pnl_day is not None:
            print(
                f"Meilleur jour: ${max_pnl_day['PnlAfterFiltering']:.2f} le {format_date(max_pnl_day['session_date'])}")

        if min_pnl_day is not None:
            print(f"Pire jour: ${min_pnl_day['PnlAfterFiltering']:.2f} le {format_date(min_pnl_day['session_date'])}")
    else:
        print("Aucune donn√©e disponible pour l'analyse")

    return fig


def correlation_matrices(X, y, figsize=(24, 10), save_path=None):
    """
    Calcule et affiche les matrices de corr√©lation de Pearson et Spearman,
    ainsi que les corr√©lations de chaque feature avec la variable cible.
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.gridspec as gridspec
    import matplotlib.colors as mcolors

    # Calcul des matrices de corr√©lation
    pearson_corr = X.corr(method='pearson')
    spearman_corr = X.corr(method='spearman')

    # Calcul des corr√©lations avec la cible
    pearson_target = pd.DataFrame({
        'target_correlation': X.corrwith(y, method='pearson')
    })

    spearman_target = pd.DataFrame({
        'target_correlation': X.corrwith(y, method='spearman')
    })

    # Configuration de la figure avec GridSpec pour avoir une disposition personnalis√©e
    fig = plt.figure(figsize=figsize)
    gs = gridspec.GridSpec(1, 5, width_ratios=[2, 0.15, 2, 0.15, 0.05])

    # D√©finition de la colormap pour r√©utilisation
    cmap = sns.color_palette("coolwarm", as_cmap=True)

    # Matrice de Pearson
    ax_pearson = fig.add_subplot(gs[0])
    mask_pearson = np.triu(np.ones_like(pearson_corr, dtype=bool))
    sns.heatmap(pearson_corr, mask=mask_pearson, annot=True, fmt=".2f", cmap=cmap,
                square=True, linewidths=.5, annot_kws={"size": 9},
                cbar=False, ax=ax_pearson)
    ax_pearson.set_title('Matrice de corr√©lation de Pearson', fontsize=16)
    ax_pearson.set_xticklabels(ax_pearson.get_xticklabels(), rotation=45, ha='right', fontsize=9)
    ax_pearson.set_yticklabels(ax_pearson.get_yticklabels(), fontsize=9)

    # Corr√©lation cible - Pearson
    ax_target_p = fig.add_subplot(gs[1])
    sns.heatmap(pearson_target, annot=True, fmt=".2f", cmap=cmap,
                linewidths=.5, annot_kws={"size": 9},
                cbar=False, ax=ax_target_p)
    ax_target_p.set_title('Corr√©lation\navec Y', fontsize=12)
    ax_target_p.set_xticklabels([])  # Pas de labels X
    ax_target_p.set_yticklabels([])  # Suppression des labels Y

    # Matrice de Spearman
    ax_spearman = fig.add_subplot(gs[2])
    mask_spearman = np.triu(np.ones_like(spearman_corr, dtype=bool))
    sns.heatmap(spearman_corr, mask=mask_spearman, annot=True, fmt=".2f", cmap=cmap,
                square=True, linewidths=.5, annot_kws={"size": 9},
                cbar=False, ax=ax_spearman)
    ax_spearman.set_title('Matrice de corr√©lation de Spearman', fontsize=16)
    ax_spearman.set_xticklabels(ax_spearman.get_xticklabels(), rotation=45, ha='right', fontsize=9)
    ax_spearman.set_yticklabels([])  # Pas de labels Y

    # Corr√©lation cible - Spearman
    ax_target_s = fig.add_subplot(gs[3])
    sns.heatmap(spearman_target, annot=True, fmt=".2f", cmap=cmap,
                linewidths=.5, annot_kws={"size": 9},
                cbar=False, ax=ax_target_s)
    ax_target_s.set_title('Corr√©lation\navec Y', fontsize=12)
    ax_target_s.set_xticklabels([])  # Pas de labels X
    ax_target_s.set_yticklabels([])  # Pas de labels Y

    # Ajout d'une l√©gende commune
    ax_cb = fig.add_subplot(gs[4])
    norm = mcolors.Normalize(vmin=-1, vmax=1)
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    cbar = plt.colorbar(sm, cax=ax_cb)
    cbar.set_ticks([-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8])

    # Ajustement des espaces
    plt.tight_layout()
    fig.subplots_adjust(bottom=0.25, wspace=0.05)

    # Sauvegarde de la figure si un chemin est sp√©cifi√©
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

    # Affichage des corr√©lations tri√©es avec la variable cible
    pearson_target_sorted = pearson_target.sort_values('target_correlation', ascending=False)
    spearman_target_sorted = spearman_target.sort_values('target_correlation', ascending=False)

    print("Corr√©lations de Pearson avec la variable cible (tri√©es):")
    print(pearson_target_sorted)
    print("\nCorr√©lations de Spearman avec la variable cible (tri√©es):")
    print(spearman_target_sorted)

    # Retourne les r√©sultats dans un dictionnaire
    return {
        'pearson_matrix': pearson_corr,
        'spearman_matrix': spearman_corr,
        'pearson_target': pearson_target,
        'spearman_target': spearman_target
    }


import numpy as np
import pandas as pd


def compute_stoch(high, low, close, session_starts, k_period=14, d_period=3, fill_value=50):
    """
    Calcule l'oscillateur stochastique (%K et %D) en respectant les limites de chaque session.
    Version optimis√©e utilisant des op√©rations vectoris√©es.

    Parameters:
    -----------
    high : array-like
        S√©rie des prix les plus hauts
    low : array-like
        S√©rie des prix les plus bas
    close : array-like
        S√©rie des prix de fermeture
    session_starts : array-like (bool√©en)
        Indicateur de d√©but de session (True lorsqu'une nouvelle session commence)
    k_period : int, default=14
        P√©riode pour calculer le stochastique %K
    d_period : int, default=3
        P√©riode pour la moyenne mobile du %K qui donne le %D
    fill_value : float, default=50
        Valeur par d√©faut pour remplacer les NaN ou divisions par z√©ro

    Returns:
    --------
    tuple
        (k_values, d_values) - Un tuple contenant les valeurs %K et %D
    """
    # Cr√©er un DataFrame pour traitement
    df = pd.DataFrame({
        'high': high,
        'low': low,
        'close': close,
        'session_start': session_starts
    })

    # Cr√©er identifiant de session
    df['session_id'] = df['session_start'].cumsum()

    # Indexer chaque barre dans sa session pour filtrage ult√©rieur
    df['bar_index_in_session'] = df.groupby('session_id').cumcount()

    # Calcul vectoris√© des plus hauts et plus bas sur la p√©riode k_period
    df['highest_high'] = (
        df.groupby('session_id')['high']
        .rolling(window=k_period, min_periods=1)
        .max()
        .reset_index(level=0, drop=True)
    )

    df['lowest_low'] = (
        df.groupby('session_id')['low']
        .rolling(window=k_period, min_periods=1)
        .min()
        .reset_index(level=0, drop=True)
    )

    # Calculer %K (Stochastique Rapide) vectoris√©
    denominator = df['highest_high'] - df['lowest_low']
    df['%K'] = np.where(
        denominator > 0,
        ((df['close'] - df['lowest_low']) / denominator) * 100,
        fill_value
    )

    # Marquer les positions n'ayant pas assez d'historique avec la valeur par d√©faut
    df.loc[df['bar_index_in_session'] < (k_period - 1), '%K'] = fill_value

    # Calculer %D (moyenne mobile du %K) vectoris√© par session
    df['%D'] = (
        df.groupby('session_id')['%K']
        .rolling(window=d_period, min_periods=1)
        .mean()
        .reset_index(level=0, drop=True)
    )

    # G√©rer les positions n'ayant pas assez d'historique pour %D
    # (k_period - 1 + d_period - 1) points n√©cessaires au total
    df.loc[df['bar_index_in_session'] < (k_period + d_period - 2), '%D'] = fill_value

    # Gestion des NaN
    df['%K'] = df['%K'].fillna(fill_value)
    df['%D'] = df['%D'].fillna(fill_value)

    # Limiter aux valeurs valides du stochastique (entre 0 et 100)
    df['%K'] = np.clip(df['%K'], 0, 100)
    df['%D'] = np.clip(df['%D'], 0, 100)

    # Retourner les valeurs sous forme de numpy arrays
    return df['%K'].to_numpy(), df['%D'].to_numpy()


def compute_wr(high, low, close, session_starts, period=14, fill_value=-50):
    """
    Calcule l'indicateur Williams %R en respectant les limites de chaque session.
    Version optimis√©e utilisant des op√©rations vectoris√©es.
    """
    # Cr√©er un DataFrame pour traitement
    df = pd.DataFrame({
        'high': high,
        'low': low,
        'close': close,
        'session_start': session_starts
    })

    # Cr√©er identifiant de session
    df['session_id'] = df['session_start'].cumsum()

    # Calcul du rolling max et min par session
    df['highest_high'] = (
        df.groupby('session_id')['high']
        .rolling(window=period, min_periods=1)
        .max()
        .reset_index(level=0, drop=True)
    )

    df['lowest_low'] = (
        df.groupby('session_id')['low']
        .rolling(window=period, min_periods=1)
        .min()
        .reset_index(level=0, drop=True)
    )

    # Calculer le Williams %R vectoris√©
    denominator = df['highest_high'] - df['lowest_low']
    df['wr'] = np.where(
        denominator > 0,
        ((df['highest_high'] - df['close']) / denominator) * -100,
        fill_value
    )

    # Identifier les positions dans chaque session qui n'ont pas assez d'historique
    df['bar_index_in_session'] = df.groupby('session_id').cumcount()
    df.loc[df['bar_index_in_session'] < (period - 1), 'wr'] = fill_value

    # Gestion des NaN
    df['wr'] = df['wr'].fillna(fill_value)

    # Limiter aux valeurs valides du Williams %R (entre -100 et 0)
    df['wr'] = np.clip(df['wr'], -100, 0)

    return df['wr'].to_numpy()

import pandas as pd
import numpy as np

def compute_mfi(
    high,
    low,
    close,
    volume,
    session_starts,
    period=14,
    fill_value=50
):
    """
    Calcule l'indicateur Money Flow Index (MFI) en r√©initialisant le calcul
    √† chaque nouvelle session, sans d√©border sur la session pr√©c√©dente.

    Parameters
    ----------
    high : array-like
        S√©ries des prix les plus hauts
    low : array-like
        S√©ries des prix les plus bas
    close : array-like
        S√©ries des prix de cl√¥ture
    volume : array-like
        S√©ries des volumes
    session_starts : array-like de bool
        Indique, pour chaque barre, si c'est le d√©but d'une nouvelle session (True) ou non (False)
    period : int, default=14
        P√©riode de calcul du MFI
    fill_value : float, default=50
        Valeur par d√©faut √† utiliser lorsque le MFI n'est pas calculable (ex: d√©but de session ou NaN)

    Returns
    -------
    np.ndarray
        Tableau des valeurs du MFI, r√©initialis√© √† chaque session
    """

    # Convertit tous les inputs en Series align√©es sur le m√™me index
    df = pd.DataFrame({
        'high'          : high,
        'low'           : low,
        'close'         : close,
        'volume'        : volume,
        'session_starts': session_starts
    })

    # Identifiants de session (on incr√©mente de 1 √† chaque True)
    # Exemple : [F, F, T, F, F, T, F] -> [0, 0, 1, 1, 1, 2, 2]
    df['session_id'] = df['session_starts'].cumsum()

    # Typical Price
    df['tp'] = (df['high'] + df['low'] + df['close']) / 3

    # Money Flow brut
    df['mf'] = df['tp'] * df['volume']

    # On compare la typical price avec celle de la barre pr√©c√©dente (shift)
    df['tp_shifted'] = df['tp'].shift(1).fillna(df['tp'].iloc[0] if len(df) else 0)

    # D√©terminer la partie positive/n√©gative du flux
    df['positive_flow'] = np.where(df['tp'] > df['tp_shifted'], df['mf'], 0)
    # On ajoute une tr√®s petite valeur pour √©viter d'avoir 0 exact
    df['negative_flow'] = np.where(df['tp'] < df['tp_shifted'], df['mf'], 0) + 1e-10

    # Rolling sum par session_id
    # -> groupby('session_id').rolling(window=period) ...
    df['sum_positive'] = (
        df.groupby('session_id')['positive_flow']
          .rolling(window=period, min_periods=1)
          .sum()
          .reset_index(level=0, drop=True)
    )
    df['sum_negative'] = (
        df.groupby('session_id')['negative_flow']
          .rolling(window=period, min_periods=1)
          .sum()
          .reset_index(level=0, drop=True)
    )

    # Ratio
    df['mfr'] = df['sum_positive'] / df['sum_negative'].clip(lower=1e-10)
    df['mfi'] = 100 - (100 / (1.0 + df['mfr']))

    # √Ä l'int√©rieur d'une session, pour les premi√®res barres (< period), on a trop peu d'historique
    # => on force ces valeurs √† fill_value
    # cumcount() num√©rote les lignes de chaque session, √† partir de 0
    # si cumcount() < period-1, on n'a pas assez de barres pour un 'vrai' MFI
    df['bar_index_in_session'] = df.groupby('session_id').cumcount()
    df.loc[df['bar_index_in_session'] < (period - 1), 'mfi'] = fill_value

    # Remplacer √©ventuellement les NaN restants par fill_value
    df['mfi'] = df['mfi'].fillna(fill_value)

    # Clip [0, 100]
    df['mfi'] = np.clip(df['mfi'], 0, 100)

    return df['mfi'].to_numpy()
